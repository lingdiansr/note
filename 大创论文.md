# **自然语言实体识别**

## Google Scholar

### 嵌套NER

- Wang, Y., Shindo, H., Matsumoto, Y., & Watanabe, T. (2022). Nested named entity recognition via explicitly excluding the influence of the best path. *Journal of Natural Language Processing*, *29*(1), 23-52.
  - 通过显式排除最优路径影响的嵌套命名实体识别
  - 在每个字/词处理时维护一组隐藏状态，并选择性地利用这些隐藏状态来构建不同级别的识别势函数：计算-选择-更新
  - 编码：三层双向LSTM对句子进行编码 将隐藏状态分割成多个块（chunks）
  - 解码：条件随机场（CRF）
  - 数据集：在ACE2004、ACE2005、GENIA和NNE四个标准基准数据集上进行了实验。
  - BERT与 Flair结合 
- Shen, Y., Ma, X., Tan, Z., Zhang, S., Wang, W., & Lu, W. (2021). Locate and label: A two-stage identifier for nested named entity recognition. *arxiv preprint arxiv:2105.06804*.
  - 定位与标注：一种用于嵌套命名实体识别的两阶段标识器
  - 将NER任务视为边界回归和跨度分类的联合任务，通过两个阶段的处理来提高嵌套实体的识别能力
  - **生成跨度提议（Span Proposal）**
    - **过滤器（Filter）**：通过过滤器将种子跨度（seed spans）分为上下文跨度（contextual spans）和提议跨度（proposal spans），过滤掉低质量的上下文跨度，以减少候选跨度的数量。
    - **边界回归器（Regressor）**：通过调整提议跨度的边界，提高候选跨度的质量，从而更准确地定位实体。
  - **实体分类（Entity Classification）**
    - 使用实体分类器对调整后的提议跨度进行分类，标注其对应的类别。
  - 通过基于IoU（Intersection over Union）的加权损失函数，将部分匹配的跨度视为软示例，充分利用这些跨度的信息。使用焦点损失（focal loss）解决正负样本不平衡问题。
  - 编码：使用软非极大值抑制（Soft-NMS）算法过滤假阳性，提取最终的实体。
  - 在多个嵌套NER数据集（如KBP17、ACE04和ACE05）上，该模型的表现优于之前的最先进模型，F1分数分别提高了3.08%、0.71%和1.27%。

### 医学领域

- Kong, J., Zhang, L., Jiang, M., & Liu, T. (2021). Incorporating multi-level CNN and attention mechanism for Chinese clinical named entity recognition. *Journal of Biomedical Informatics*, *116*, 103737. 
  - 结合多级卷积神经网络和注意力机制的中文临床命名实体识别
  - 旨在解决传统长短期记忆网络（LSTM）在处理长句子时效率低下和难以捕获全局上下文信息的问题。通过利用CNN的并行计算能力和注意力机制的全局信息捕获能力，模型能够更高效地处理临床文本数据，提高命名实体识别的准确性和效率。
  - 通过在训练数据集中交换相同类型的实体来生成新的训练数据集，从而扩增数据量并增强模型的泛化能力。
  - 使用预训练的嵌入（如GloVe和Word2Vec）生成每个字符的不同嵌入表示，并将这些嵌入拼接成多模态嵌入，以获取更丰富的语义信息。

- Naseem, U., Khushi, M., Reddy, V., Rajendran, S., Razzak, I., & Kim, J. (2021, July). Bioalbert: A simple and effective pre-trained language model for biomedical named entity recognition. In *2021 International Joint Conference on Neural Networks (IJCNN)* (pp. 1-7). IEEE.

  - Bioalbert: 一种用于生物医学命名实体识别的简单而有效的预训练语言模型
  - BioALBERT基于ALBERT模型架构，采用自监督损失函数，专注于建模句子间的连贯性，以更好地学习上下文依赖表示。
  - 引入参数减少技术，包括跨层参数共享和因子分解嵌入参数化，以降低内存消耗并提高训练速度。
  - 在八个生物医学命名实体识别基准数据集上进行微调，这些数据集包含疾病、物种、药物/蛋白质和药物/化学物质等四类实体。
  - 使用adamw优化器，批量大小为32，学习率为1e-5，训练步数为5336步，采用BIO标注方案（B、I、O）进行实体标注。
  - 通过跨层参数共享显著减少了参数数量，同时保持了相同的层数和隐藏单元。
  - 使用句子顺序预测（SOP）技术，使模型能够学习到更细粒度的连贯性特征，从而在下游任务中表现更好。
  - 通过因子分解嵌入参数化，减少了词汇嵌入和隐藏层之间的参数数量，提高了模型的参数效率。

- An, Y., **a, X., Chen, X., Wu, F. X., & Wang, J. (2022). Chinese clinical named entity recognition via multi-head self-attention based BiLSTM-CRF. *Artificial intelligence in medicine*, *127*, 102282. [文章](https://www.sciencedirect.com/science/article/abs/pii/S0933365722000471)

  - 基于多头自注意力的BiLSTM-CRF用于中文临床命名实体识别
  - 基于多头自注意力的BiLSTM-CRF（MUSA-BiLSTM-CRF），旨在解决现有模型在利用临床文本的全局信息和多层次语义特征方面的不足。通过改进的字符级特征表示方法和引入多头自注意力机制，模型能够更有效地捕获字符之间的权重关系和多层次语义特征信息
  - 双向长短期记忆网络（BiLSTM）捕获输入序列的时间特征和上下文依赖关系，使模型能够有效地学习文本的上下文语义信息
  - 多头自注意力机制，获取临床句子中每个字符在多个时间步的加权表示，使模型能够更准确地关注句子中的相关字符或单词，从而提高实体识别的准确性
  - [FlashMLA](https://github.com/deepseek-ai/FlashMLA) HoperGPU/国产GPU

- Deng, N., Fu, H., & Chen, X. (2021). Named Entity Recognition of Traditional Chinese Medicine Patents Based on BiLSTM‐CRF. *Wireless Communications and Mobile Computing*, *2021*(1), 6696205. 

  - 基于BiLSTM-CRF的传统中医专利命名实体识别
  - 利用深度学习中的BiLSTM-CRF模型来自动识别传统中医专利文本中的关键实体，如草药名称、疾病名称、症状和治疗效果。通过结合BiLSTM和CRF的优势，模型能够充分利用上下文信息，提高命名实体识别的准确性。这一方法旨在解决传统中医专利文本中命名实体识别的挑战，如草药名称的多义性、疾病名称的表达多样性以及缺乏标注训练数据等问题。
  - **数据**：从知识产权网站爬取了2000份传统中医专利的摘要文本，通过正则表达式和字符格式标准化技术，去除了非文本数据，最终保留了1600份摘要文本作为实验语料。将数据集划分为训练集、验证集和测试集，比例为6:2:2。
  - BIO标注方法：采用BIO（Begin, Intermediate, Other）标注方法对文本进行标注，其中“B”表示词的首字符，“I”表示词的非首字符，“O”表示非关注字符或标点。
  - 实体类别：主要识别四类实体，即草药名称、疾病名称、症状和治疗效果。
  - **模型**：
    - **输入层**：将每个字符映射为低维密集向量，使用预训练的嵌入矩阵。
    - **嵌入层**：将字符转换为向量，以便计算机进行计算。使用Word2Vec模型进行预训练，并在反向传播过程中进行微调。
    - **BiLSTM层**：用于提取句子特征。通过双向LSTM网络，捕获前后文信息，获得完整的隐藏状态序列。
    - **CRF层**：进行句子级别的序列标注，确保生成全局最优的标注序列。通过学习序列之间的潜在关系，排除不符合句子级语法规则的标注序列。

  - BiLSTM-CRF模型的精确率为94.63%，召回率为94.47%，F1值为94.48%。与HMM、LSTM和BiLSTM模型相比，BiLSTM-CRF模型在准确率、召回率和F1值上均表现更优。
  - **草药名称**的识别效果最好，**疾病名称**和**治疗效果**的识别效果也较为理想，而**症状**的识别效果相对较差，主要是因为症状实体的标注较少且容易与其他实体类别混淆。

- Gao, S., Kotevska, O., Sorokine, A., & Christian, J. B. (2021). A pre-training and self-training approach for biomedical named entity recognition. *PloS one*, *16*(2), e0246310. 

  - 一种用于生物医学命名实体识别的预训练和自我训练方法
  - 结合预训练和自我训练的方法，以提高生物医学领域命名实体识别（NER）模型在标注数据有限情况下的性能。通过在大规模通用生物医学NER语料库上预训练模型，然后在目标任务上进行微调，最后利用半监督自我训练进一步提升模型性能，该方法能够在标注数据较少的情况下达到与全监督模型相近的性能。
  - 预训练模型包括BiLSTM-CRF和BERT，大规模通用生物医学NER语料库（如MedMentions和Semantic Medline）
  - 自我训练
    - 利用半监督学习中的自我训练方法，进一步提升模型性能。
    - 在自我训练过程中，模型首先对未标注数据进行预测，然后根据预测的置信度筛选出高质量的伪标签，将这些伪标签加入训练集，再次训练模型。
    - 重复这一过程，直到没有更多的未标注数据符合置信度阈值。

- Kocaman, V., Talby, D. (2021). Biomedical Named Entity Recognition at Scale. In: Del Bimbo, A., *et al.* Pattern Recognition. ICPR International Workshops and Challenges. ICPR 2021. Lecture Notes in Computer Science(), vol 12661. Springer, Cham. https://doi.org/10.1007/978-3-030-68763-2_48

  - 大规模生物医药命名实体识别
  - 基于对Bi-LSTM-CNN-Char深度学习架构的重新实现，并将其应用于Apache Spark环境。这种方法能够在不使用像BERT那样计算资源密集型的上下文嵌入的情况下，获得新的最先进的结果。
  - 于Bi-LSTM-CNN-Char框架，这是一种混合双向LSTM和CNN的神经网络架构，能够自动检测词级和字符级特征，从而消除了大多数特征工程步骤。在Spark NLP中，作者对这一框架进行了修改，例如使用TensorFlow实现了一个修改版的框架，并使用LSTMBlockFusedCell以提高效率。
  - **词嵌入**：作者训练了自己的生物医学词嵌入，使用skip-gram模型在PubMed摘要和病例研究上进行训练，得到200维的词嵌入，词汇量大小为220万。同时，也使用了300维的预训练GloVe嵌入进行对比。
  - **字符嵌入**：通过字符级别的卷积神经网络（CNN）来提取字符特征，每个字符被嵌入到一个25维的字符嵌入矩阵中，然后通过1D卷积层和最大池化操作处理嵌入的字符向量序列，为每个单词生成向量表示。
  - **模型训练**：在8个公开的生物医学NER数据集上进行了训练，并通过随机搜索对超参数进行了调整，包括LSTM状态大小、丢弃率、批量大小、学习率、学习率衰减系数等。训练过程中使用了Adam优化器。

  

### 其他

- Su, J., Murtadha, A., Pan, S., Hou, J., Sun, J., Huang, W., ... & Liu, Y. (2022). Global pointer: Novel efficient span-based approach for named entity recognition. *arxiv preprint arxiv:2208.03054*.
  - 全局指针：一种新的高效的基于跨度的命名实体识别方法
  - 利用相对位置信息和乘法注意力机制，旨在实现全局视角，同时考虑实体的开始和结束位置来预测实体。核心思路是设计两个模块来识别给定实体的头部和尾部，以解决训练和推理过程中的不一致问题，并引入一种新的分类损失函数来应对标签不平衡问题。
  - **步骤**
    - **Token表示**：给定一个句子，使用预训练语言模型（如BERT）为每个词生成对应的表示，得到一个表示矩阵。
    - **跨度预测**：通过两个前馈层，基于跨度的开始和结束索引计算跨度表示。对于每个可能的跨度，计算其作为特定类型实体的分数。
    - **相对位置信息**：将相对位置信息显式注入模型，使用旋转位置编码（ROPE）来增强位置信息的表达能力。
    - **分类损失函数**：引入一种新的损失函数，基于circle loss的思想，旨在使目标类别的分数不低于非目标类别的分数，以缓解类别不平衡问题。
    - **参数减少**：提出了一种近似方法来减少训练参数的数量，特别是在添加新实体类型时。通过将NER任务分解为提取和分类两个子任务，共享提取步骤的参数，从而减少参数数量。

- Zhou, W., Zhang, S., Gu, Y., Chen, M., & Poon, H. (2023). Universalner: Targeted distillation from large language models for open named entity recognition. *arxiv preprint arxiv:2308.03279*.
  - 通用命名实体识别：从大型语言模型中进行针对性蒸馏以实现开放域命名实体识别
  - 以命名实体识别（NER）为案例，展示了如何从ChatGPT中蒸馏出更小的UniversalNER模型，使其在多个领域和实体类型上表现出色，甚至超越了原始的大型语言模型。
  - 生成对抗网络-判别器







